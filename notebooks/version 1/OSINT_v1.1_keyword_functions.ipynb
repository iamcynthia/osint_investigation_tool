{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/paddle/fluid/layers/utils.py:26: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  def convert_to_list(value, n, name, dtype=np.int):\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import requests\n",
    "import math\n",
    "from lxml import html\n",
    "from collections import Counter\n",
    "from string import punctuation as enPunc\n",
    "from zhon.hanzi import punctuation as zhPunc\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from elasticsearch import Elasticsearch\n",
    "from bs4  import BeautifulSoup\n",
    "from LAC import LAC\n",
    "from opencc import OpenCC\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## 計算 org+title+per, org+per, org, per 的詞頻\n",
    "- ner區：某些詞組不到、group ner頻率通常是1、會受組合的dataframe順序影響最後排序結果\n",
    "- 文章區："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def getFinalText(google_search_rs):\n",
    "    html_text = []\n",
    "    getLink = []\n",
    "    for idx, link in enumerate(google_search_rs):\n",
    "        getLink.append((idx+1, link))\n",
    "        print(idx, link)\n",
    "        if 'pdf' in link or 'PDF' in link or 'download' in link or 'Download' in link:\n",
    "            html_text.append('')\n",
    "            continue\n",
    "        try:\n",
    "            resp = requests.get(link, headers={'user-agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.141 Safari/537.36'})\n",
    "            soup = BeautifulSoup(resp.content, 'html.parser')\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "        a_tag = soup.find_all('a')\n",
    "        for a in a_tag:\n",
    "            a.decompose()   \n",
    "        html_text.append(soup.get_text()) \n",
    "    \n",
    "    # 繁轉簡\n",
    "    cc = OpenCC('tw2sp')\n",
    "    temp = [cc.convert(texts) for texts in html_text]\n",
    "\n",
    "    # 去除英文\n",
    "    final_text = [re.sub(r'[a-zA-Z]',\"\", texts) for texts in temp]\n",
    "    return final_text, getLink\n",
    "\n",
    "def multi_repl(input_str):\n",
    "    trim = ['\\n', '\\r', '\\t', '\\xa0', ' '] + list(enPunc)+list(zhPunc) ## 網頁亂碼+英文標點+中文標點\n",
    "    trim_dict = {pat:'' for pat in trim}\n",
    "    for k in trim_dict:\n",
    "        input_str = input_str.replace(k, trim_dict[k])  ## 把stop words改為空白\n",
    "    return input_str\n",
    "\n",
    "def stopwords():\n",
    "    # Load stop words\n",
    "    f = open('/Users/csti-user/Downloads/NER/stopwords-master/baidu_stopwords.txt','r')\n",
    "    k = f.readlines()\n",
    "    f.close()\n",
    "    stopwords = [lines.strip('\\n') for lines in k]\n",
    "\n",
    "    # 建構stopwords\n",
    "    trim = ['\\n', '\\r', '\\t', '\\xa0'] + list(enPunc)+list(zhPunc) ## 網頁亂碼+英文標點+中文標點\n",
    "    trim = trim + stopwords  # list of all stopwords\n",
    "    return trim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## Data Format\n",
    "def ner_combine(ner_pair):\n",
    "    for idx, (word, ner, cat) in enumerate(ner_pair):\n",
    "        if (idx + 1) == len(ner_pair) or (idx + 2) == len(ner_pair):\n",
    "            break\n",
    "        else:  # 直接將組合詞加入原先分詞好的list (ner_pair)\n",
    "            n1_ner = ner_pair[idx+1]\n",
    "            if cat == n1_ner[2]:\n",
    "                n2_ner = ner_pair[idx+2]\n",
    "                if ner == 'ORG' and n1_ner[1] == 'PER':\n",
    "                    new_wd = (word+n1_ner[0],'ORG+PER', cat)\n",
    "                    del ner_pair[idx:idx+2]\n",
    "                    ner_pair.insert(idx,new_wd)\n",
    "                elif ner == 'ORG' and n1_ner[1] == 'TITLE' and n2_ner[1] == 'PER':\n",
    "                    new_wd = (word+n1_ner[0]+n2_ner[0],'ORG+TITLE+PER', cat)\n",
    "                    del ner_pair[idx:idx+3]\n",
    "                    ner_pair.insert(idx,new_wd)\n",
    "\n",
    "def ner_extract(final_text):\n",
    "    lac = LAC(mode='lac')  \n",
    "    lac.load_customization('custom_dict.txt')  # 自行增加字典\n",
    "    ner_model = lac.run(final_text)\n",
    "\n",
    "    ner_pair = []\n",
    "    ner_wd = []\n",
    "    trim = stopwords()\n",
    "    for doc, (word, ner) in enumerate(ner_model):\n",
    "        temp = []\n",
    "        for num in range(len(word)):\n",
    "            if word[num] not in trim: # remove stop words\n",
    "                word[num] = multi_repl(word[num])\n",
    "                if word[num] != '' and ner[num] != '' and ner[num] != 'f' and ner[num] != 'nw' and ner[num] != 'm':\n",
    "                    temp.append(word[num])\n",
    "                    ner_pair.append((word[num],ner[num],doc+1))\n",
    "        ner_wd.append(\" \".join(str(x) for x in temp))\n",
    "    ner_combine(ner_pair)\n",
    "    return ner_pair, ner_wd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## ner區\n",
    "def auto_extract(ner_pair):\n",
    "    df = pd.DataFrame(ner_pair,columns=['words','ner','cat'])  # 選定文章的分詞結果\n",
    "    keywords = pd.concat((df[df['ner'] == 'ORG+TITLE+PER'],df[df['ner'] == 'PER'], \n",
    "                          df[df['ner'] == 'ORG+PER'], df[df['ner'] == 'ORG']), ignore_index=True)\n",
    "    ## 15 most common words dataframe\n",
    "    cnt = pd.DataFrame(Counter(keywords.words).most_common(15),columns=['words', 'cnt'])\n",
    "    return cnt\n",
    "\n",
    "## 文章區\n",
    "def article(ner_pair):\n",
    "    df = pd.DataFrame(ner_pair,columns=['words','ner','cat'])  # 選定文章的分詞結果\n",
    "    keywords = pd.concat((df[df['ner'] == 'ORG+TITLE+PER'],df[df['ner'] == 'PER'], \n",
    "                          df[df['ner'] == 'ORG+PER'], df[df['ner'] == 'ORG'],\n",
    "                         df[df['ner'] == 'TITLE'])).sort_index()\n",
    "    keywords = keywords.reset_index()\n",
    "    keywords = keywords.drop(columns =['index'])\n",
    "    cnt = auto_extract(ner_pair)\n",
    "    cntList = cnt.words.unique().tolist()\n",
    "    \n",
    "    idx = [i for i in range(len(keywords)) if keywords.words.iloc[i] in cntList]\n",
    "    keywords = keywords.drop(idx).reset_index()\n",
    "    keywords = keywords.drop(columns=['index', 'cat'])\n",
    "    return keywords    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 http://www.bast.net.cn/art/2019/12/25/art_23348_448376.html\n"
     ]
    }
   ],
   "source": [
    "## commands\n",
    "url = ['http://www.bast.net.cn/art/2019/12/25/art_23348_448376.html'] # select one url\n",
    "final_text, link = getFinalText(url)\n",
    "ner_pair, ner_wd = ner_extract(final_text)\n",
    "cnt = auto_extract(ner_pair)\n",
    "keywords = article(ner_pair)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## 計算 PER 的詞頻\n",
    "- NER: 難以分辨每個人名的重要性\n",
    "- Content: need to rethink"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def getFinalText(google_search_rs):\n",
    "    html_text = []\n",
    "    getLink = []\n",
    "    for idx, link in enumerate(google_search_rs):\n",
    "        getLink.append((idx+1, link))\n",
    "        print(idx, link)\n",
    "        if 'pdf' in link or 'PDF' in link or 'download' in link or 'Download' in link:\n",
    "            html_text.append('')\n",
    "            continue\n",
    "        try:\n",
    "            resp = requests.get(link, headers={'user-agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.141 Safari/537.36'})\n",
    "            soup = BeautifulSoup(resp.content, 'html.parser')\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "        a_tag = soup.find_all('a')\n",
    "        for a in a_tag:\n",
    "            a.decompose()   \n",
    "        html_text.append(soup.get_text()) \n",
    "    \n",
    "    # 繁轉簡\n",
    "    cc = OpenCC('tw2sp')\n",
    "    temp = [cc.convert(texts) for texts in html_text]\n",
    "\n",
    "    # 去除英文\n",
    "    final_text = [re.sub(r'[a-zA-Z]',\"\", texts) for texts in temp]\n",
    "    return final_text, getLink\n",
    "\n",
    "def multi_repl(input_str):\n",
    "    trim = ['\\n', '\\r', '\\t', '\\xa0', ' '] + list(enPunc)+list(zhPunc) ## 網頁亂碼+英文標點+中文標點\n",
    "    trim_dict = {pat:'' for pat in trim}\n",
    "    for k in trim_dict:\n",
    "        input_str = input_str.replace(k, trim_dict[k])  ## 把stop words改為空白\n",
    "    return input_str\n",
    "\n",
    "def stopwords():\n",
    "    # Load stop words\n",
    "    f = open('/Users/csti-user/Downloads/NER/stopwords-master/baidu_stopwords.txt','r')\n",
    "    k = f.readlines()\n",
    "    f.close()\n",
    "    stopwords = [lines.strip('\\n') for lines in k]\n",
    "\n",
    "    # 建構stopwords\n",
    "    trim = ['\\n', '\\r', '\\t', '\\xa0'] + list(enPunc)+list(zhPunc) ## 網頁亂碼+英文標點+中文標點\n",
    "    trim = trim + stopwords  # list of all stopwords\n",
    "    return trim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## Data Format\n",
    "def ner_extract(final_text):\n",
    "    lac = LAC(mode='lac')  \n",
    "    lac.load_customization('custom_dict.txt')  # 自行增加字典\n",
    "    ner_model = lac.run(final_text)\n",
    "\n",
    "    ner_pair = []\n",
    "    ner_wd = []\n",
    "    trim = stopwords()\n",
    "    for doc, (word, ner) in enumerate(ner_model):\n",
    "        temp = []\n",
    "        for num in range(len(word)):\n",
    "            if word[num] not in trim: # remove stop words\n",
    "                word[num] = multi_repl(word[num])\n",
    "                if word[num] != '' and ner[num] != '' and ner[num] != 'f' and ner[num] != 'nw' and ner[num] != 'm':\n",
    "                    temp.append(word[num])\n",
    "                    ner_pair.append((word[num],ner[num],doc+1))\n",
    "        ner_wd.append(\" \".join(str(x) for x in temp))\n",
    "    return ner_pair, ner_wd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## ner區\n",
    "def auto_extract(ner_pair):\n",
    "    df = pd.DataFrame(ner_pair,columns=['words','ner','cat'])  # 選定文章的分詞結果\n",
    "    keywords = df[df['ner'] == 'PER']\n",
    "    ## 15 most common words dataframe\n",
    "    cnt = pd.DataFrame(Counter(keywords.words).most_common(),columns=['words', 'cnt'])\n",
    "    return cnt\n",
    "\n",
    "## 文章區\n",
    "def article(ner_pair):\n",
    "    df = pd.DataFrame(ner_pair,columns=['words','ner','cat'])  # 選定文章的分詞結果\n",
    "    keywords = pd.concat((df[df['ner'] == 'PER'], df[df['ner'] == 'ORG'], df[df['ner'] == 'TITLE'])).sort_index()\n",
    "    keywords = keywords.reset_index()\n",
    "    keywords = keywords.drop(columns =['index', 'cat'])\n",
    "#     cnt = auto_extract(ner_pair)\n",
    "#     cntList = cnt.words.unique().tolist()\n",
    "    \n",
    "#     idx = [i for i in range(len(keywords)) if keywords.words.iloc[i] in cntList]\n",
    "#     keywords = keywords.drop(idx).reset_index()\n",
    "#     keywords = keywords.drop(columns=['index', 'cat'])\n",
    "    return keywords    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 http://www.bast.net.cn/art/2019/12/25/art_23348_448376.html\n"
     ]
    }
   ],
   "source": [
    "## commands\n",
    "url = ['http://www.bast.net.cn/art/2019/12/25/art_23348_448376.html'] # select one url\n",
    "final_text, link = getFinalText(url)\n",
    "ner_pair, ner_wd = ner_extract(final_text)\n",
    "cnt = auto_extract(ner_pair)\n",
    "keywords = article(ner_pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## 計算ORG的詞頻\n",
    "- NER: 覺得只計算組織的詞頻沒太多用處\n",
    "- Content: need to rethink"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def getFinalText(google_search_rs):\n",
    "    html_text = []\n",
    "    getLink = []\n",
    "    for idx, link in enumerate(google_search_rs):\n",
    "        getLink.append((idx+1, link))\n",
    "        print(idx, link)\n",
    "        if 'pdf' in link or 'PDF' in link or 'download' in link or 'Download' in link:\n",
    "            html_text.append('')\n",
    "            continue\n",
    "        try:\n",
    "            resp = requests.get(link, headers={'user-agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.141 Safari/537.36'})\n",
    "            soup = BeautifulSoup(resp.content, 'html.parser')\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "        a_tag = soup.find_all('a')\n",
    "        for a in a_tag:\n",
    "            a.decompose()   \n",
    "        html_text.append(soup.get_text()) \n",
    "    \n",
    "    # 繁轉簡\n",
    "    cc = OpenCC('tw2sp')\n",
    "    temp = [cc.convert(texts) for texts in html_text]\n",
    "\n",
    "    # 去除英文\n",
    "    final_text = [re.sub(r'[a-zA-Z]',\"\", texts) for texts in temp]\n",
    "    return final_text, getLink\n",
    "\n",
    "def multi_repl(input_str):\n",
    "    trim = ['\\n', '\\r', '\\t', '\\xa0', ' '] + list(enPunc)+list(zhPunc) ## 網頁亂碼+英文標點+中文標點\n",
    "    trim_dict = {pat:'' for pat in trim}\n",
    "    for k in trim_dict:\n",
    "        input_str = input_str.replace(k, trim_dict[k])  ## 把stop words改為空白\n",
    "    return input_str\n",
    "\n",
    "def stopwords():\n",
    "    # Load stop words\n",
    "    f = open('/Users/csti-user/Downloads/NER/stopwords-master/baidu_stopwords.txt','r')\n",
    "    k = f.readlines()\n",
    "    f.close()\n",
    "    stopwords = [lines.strip('\\n') for lines in k]\n",
    "\n",
    "    # 建構stopwords\n",
    "    trim = ['\\n', '\\r', '\\t', '\\xa0'] + list(enPunc)+list(zhPunc) ## 網頁亂碼+英文標點+中文標點\n",
    "    trim = trim + stopwords  # list of all stopwords\n",
    "    return trim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## Data Format\n",
    "def ner_extract(final_text):\n",
    "    lac = LAC(mode='lac')  \n",
    "    lac.load_customization('custom_dict.txt')  # 自行增加字典\n",
    "    ner_model = lac.run(final_text)\n",
    "\n",
    "    ner_pair = []\n",
    "    ner_wd = []\n",
    "    trim = stopwords()\n",
    "    for doc, (word, ner) in enumerate(ner_model):\n",
    "        temp = []\n",
    "        for num in range(len(word)):\n",
    "            if word[num] not in trim: # remove stop words\n",
    "                word[num] = multi_repl(word[num])\n",
    "                if word[num] != '' and ner[num] != '' and ner[num] != 'f' and ner[num] != 'nw' and ner[num] != 'm':\n",
    "                    temp.append(word[num])\n",
    "                    ner_pair.append((word[num],ner[num],doc+1))\n",
    "        ner_wd.append(\" \".join(str(x) for x in temp))\n",
    "    return ner_pair, ner_wd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## ner區\n",
    "def auto_extract(ner_pair):\n",
    "    df = pd.DataFrame(ner_pair,columns=['words','ner','cat'])  # 選定文章的分詞結果\n",
    "    keywords = df[df['ner'] == 'ORG']\n",
    "    ## 15 most common words dataframe\n",
    "    cnt = pd.DataFrame(Counter(keywords.words).most_common(),columns=['words', 'cnt'])\n",
    "    return cnt\n",
    "\n",
    "## 文章區 -- need to rethink\n",
    "def article(ner_pair):\n",
    "    df = pd.DataFrame(ner_pair,columns=['words','ner','cat'])  # 選定文章的分詞結果\n",
    "    keywords = pd.concat((df[df['ner'] == 'PER'], df[df['ner'] == 'ORG'], df[df['ner'] == 'TITLE'])).sort_index()\n",
    "    keywords = keywords.reset_index()\n",
    "    keywords = keywords.drop(columns =['index', 'cat'])\n",
    "    return keywords    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 http://www.bast.net.cn/art/2019/12/25/art_23348_448376.html\n"
     ]
    }
   ],
   "source": [
    "## commands\n",
    "url = ['http://www.bast.net.cn/art/2019/12/25/art_23348_448376.html'] # select one url\n",
    "final_text, link = getFinalText(url)\n",
    "ner_pair, ner_wd = ner_extract(final_text)\n",
    "cnt = auto_extract(ner_pair)\n",
    "keywords = article(ner_pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 計算特定NER的 TFIDF\n",
    "1. PER\n",
    "2. ORG\n",
    "3. ORG & PER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_repl(input_str):\n",
    "    trim = ['\\n', '\\r', '\\t', '\\xa0', ' '] + list(enPunc)+list(zhPunc) ## 網頁亂碼+英文標點+中文標點 (無數字)\n",
    "    trim_dict = {pat:'' for pat in trim}\n",
    "    for k in trim_dict:\n",
    "        input_str = input_str.replace(k, trim_dict[k])  ## 把stop words改為空白\n",
    "    return input_str\n",
    "\n",
    "def getFinalText(google_search_rs):\n",
    "    html_text = []\n",
    "    getLink = []\n",
    "    for idx, link in enumerate(google_search_rs):\n",
    "        getLink.append((idx+1, link))\n",
    "        print(idx +1, link)\n",
    "        if 'pdf' in link or 'PDF' in link or 'download' in link or 'Download' in link:\n",
    "            html_text.append('無結果')\n",
    "            continue\n",
    "        try:\n",
    "            resp = requests.get(link, headers={'user-agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.141 Safari/537.36'})\n",
    "            soup = BeautifulSoup(resp.content, 'html.parser')\n",
    "            a_tag = soup.find_all('a')\n",
    "            for a in a_tag:\n",
    "                a.decompose() \n",
    "            html_text.append(soup.get_text()) \n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            html_text.append('無結果') \n",
    "                 \n",
    "    \n",
    "    # 繁轉簡\n",
    "    cc = OpenCC('tw2sp')\n",
    "    temp = [cc.convert(texts) for texts in html_text]\n",
    "\n",
    "    # 只保留漢字、數字、大小寫英文\n",
    "    temp1 = [re.sub(u\"([^\\u4e00-\\u9fa5\\u0030-\\u0039])\",\"\", texts) for texts in temp]\n",
    "\n",
    "    # 去除停用詞 (先去除還是等到分詞時再去除)\n",
    "    final_text = [multi_repl(texts) for texts in temp1]\n",
    "    \n",
    "    # 紀錄資訊量少的文章 (文字少於400者)\n",
    "    lessInfo = [i+1 for i in range(len(final_text)) if len(final_text[i]) < 400]\n",
    "    \n",
    "    return final_text, link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data Format\n",
    "def ner_extract(final_text):\n",
    "    lac = LAC(mode='lac')  \n",
    "    lac.load_customization('custom_dict.txt')  # 自行增加字典\n",
    "    ner_model = lac.run(final_text)\n",
    "\n",
    "    ner_pair = []\n",
    "    ner_wd = []\n",
    "    trim = stopwords()\n",
    "    for doc, (word, ner) in enumerate(ner_model):\n",
    "        temp = []\n",
    "        for num in range(len(word)):\n",
    "            if word[num] not in trim: # remove stop words\n",
    "                word[num] = multi_repl(word[num])\n",
    "                if word[num] != '' and ner[num] != '' and ner[num] != 'f' and ner[num] != 'nw' and ner[num] != 'm':\n",
    "                    temp.append(word[num])\n",
    "                    ner_pair.append((word[num],ner[num],doc+1))\n",
    "        ner_wd.append(\" \".join(str(x) for x in temp))\n",
    "    return ner_pair, ner_wd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getIdx(url):\n",
    "    df = pd.DataFrame(getLink, columns=['idx', 'link'])\n",
    "    for i in range(len(df)):\n",
    "        if df.iloc[i,1] == url[0]:\n",
    "            return i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ner區\n",
    "def auto_extract(doc_terms_freq):\n",
    "    wd = list(df_specific[df_specific['cat']==getIdx(url)+1]['words'])\n",
    "    df_tfidf = pd.DataFrame(columns=['word','tfidf'])\n",
    "    for i, count in enumerate(doc_terms_freq):\n",
    "        if i == getIdx(url):\n",
    "            for word in count:\n",
    "                if word in wd:\n",
    "                    df_tfidf = df_tfidf.append({'word':word,'tfidf': tf(word,count)*idf(word,doc_terms_freq)},\n",
    "                                               ignore_index=True)\n",
    "    df_tfidf = df_tfidf.sort_values(by=['tfidf'], ascending=False).reset_index()\n",
    "    df_tfidf = df_tfidf.drop(columns=['index'])  ## 程式產出結果\n",
    "    return df_tfidf[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "## calculate idf\n",
    "def needs():\n",
    "    google_search_rs = pd.read_excel('ner_sample.xlsx').url.to_list()\n",
    "    google_search_text, getLink = getFinalText(google_search_rs)\n",
    "    ner_pair_S, ner_wd_S = ner_extract(google_search_text)\n",
    "\n",
    "    df_all_pair = pd.DataFrame(ner_pair_S,columns=['words','ner','cat'])\n",
    "    \n",
    "#     df_specific = pd.concat((df_all_pair[df_all_pair['ner'] == 'ORG'], df_all_pair[df_all_pair['ner'] == 'PER'])).sort_index()\n",
    "#     df_specific = df_specific.reset_index()\n",
    "\n",
    "    df_specific = df_all_pair[df_all_pair['ner'] == 'PER'].reset_index()  # select PER or ORG\n",
    "    \n",
    "    df_specific = df_specific.drop(columns=['index'])\n",
    "\n",
    "    doc_terms_freq = []  # 每篇文章每個詞的頻率\n",
    "    for i in range(len(google_search_text)):\n",
    "        tempDF = df_all_pair[df_all_pair['cat']==i+1]\n",
    "        cnt = Counter(tempDF.words)\n",
    "        doc_terms_freq.append(cnt)\n",
    "    return df_specific, doc_terms_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "#计算count_list有多少个文件包含word\n",
    "def n_containing(word, count_list):\n",
    "    return sum(1 for count in count_list if word in count)\n",
    "\n",
    "# 计算tf\n",
    "def tf(word, count):\n",
    "    return count[word] / sum(count.values())\n",
    "\n",
    "# 计算idf\n",
    "def idf(word, count_list):\n",
    "    return math.log(len(count_list) / (n_containing(word, count_list)))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-06-15 16:39:35,123 - WARNING - Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>tfidf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>北京科学技术情报学会</td>\n",
       "      <td>0.044125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>北京市科学技术协会</td>\n",
       "      <td>0.026475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>中国科学技术信息研究所</td>\n",
       "      <td>0.017650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>戴国强</td>\n",
       "      <td>0.017650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>吴晨生</td>\n",
       "      <td>0.017650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>中国科协</td>\n",
       "      <td>0.017650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>辽宁省科学技术情报研究所</td>\n",
       "      <td>0.008825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>科技部中国科技发展战略研究院调查统计中心</td>\n",
       "      <td>0.008825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>军事科学院国防科技创新研究院</td>\n",
       "      <td>0.008825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>栗琳</td>\n",
       "      <td>0.008825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>中共中央政治局</td>\n",
       "      <td>0.008825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>刘耀</td>\n",
       "      <td>0.008825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>马伟群</td>\n",
       "      <td>0.008825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>国家科学技术部</td>\n",
       "      <td>0.008825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>胥和平</td>\n",
       "      <td>0.008825</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    word     tfidf\n",
       "0             北京科学技术情报学会  0.044125\n",
       "1              北京市科学技术协会  0.026475\n",
       "2            中国科学技术信息研究所  0.017650\n",
       "3                    戴国强  0.017650\n",
       "4                    吴晨生  0.017650\n",
       "5                   中国科协  0.017650\n",
       "6           辽宁省科学技术情报研究所  0.008825\n",
       "7   科技部中国科技发展战略研究院调查统计中心  0.008825\n",
       "8         军事科学院国防科技创新研究院  0.008825\n",
       "9                     栗琳  0.008825\n",
       "10               中共中央政治局  0.008825\n",
       "11                    刘耀  0.008825\n",
       "12                   马伟群  0.008825\n",
       "13               国家科学技术部  0.008825\n",
       "14                   胥和平  0.008825"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## commands\n",
    "df_specific, doc_terms_freq = needs() # needs for calculating idf\n",
    "\n",
    "url = ['http://www.shio.gov.cn/sh/xwb/n809/n814/n1061/u1ai18626.html'] # select one url\n",
    "final_text, link = getFinalText(url)\n",
    "ner_pair, ner_wd = ner_extract(final_text)\n",
    "\n",
    "auto_extract(doc_terms_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 字典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_dict(word, ner):\n",
    "    path = '/Users/csti-user/Downloads/NER/custom_dict.txt'\n",
    "    with open(path,'a') as f:\n",
    "        f.write('%s/%s\\n'%(word,ner)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_dict('資策會', 'ORG')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
