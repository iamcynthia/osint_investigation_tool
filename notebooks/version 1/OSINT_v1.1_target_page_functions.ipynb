{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15024814",
   "metadata": {},
   "source": [
    "<b>Version 1.0 LAC做NER 計算Group NER 及 PER 的詞頻<b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f74dcdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/paddle/fluid/layers/utils.py:26: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  def convert_to_list(value, n, name, dtype=np.int):\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import requests\n",
    "import math\n",
    "from lxml import html\n",
    "from collections import Counter\n",
    "from string import punctuation as enPunc\n",
    "from zhon.hanzi import punctuation as zhPunc\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from elasticsearch import Elasticsearch\n",
    "from bs4  import BeautifulSoup\n",
    "from LAC import LAC\n",
    "from opencc import OpenCC\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce913a3",
   "metadata": {},
   "source": [
    "## 匯入資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c5b9730",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFinalText(google_search_rs):\n",
    "    html_text = []\n",
    "    getLink = []\n",
    "    for idx, link in enumerate(google_search_rs):\n",
    "        getLink.append((idx+1, link))\n",
    "#         print(idx, link)\n",
    "        if 'pdf' in link or 'PDF' in link or 'download' in link or 'Download' in link:\n",
    "            html_text.append('')\n",
    "            continue\n",
    "        try:\n",
    "            resp = requests.get(link, headers={'user-agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.141 Safari/537.36'})\n",
    "            soup = BeautifulSoup(resp.content, 'html.parser')\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "        a_tag = soup.find_all('a')\n",
    "        for a in a_tag:\n",
    "            a.decompose()   \n",
    "        html_text.append(soup.get_text()) \n",
    "    \n",
    "    # 繁轉簡\n",
    "    cc = OpenCC('tw2sp')\n",
    "    temp = [cc.convert(texts) for texts in html_text]\n",
    "\n",
    "    # 去除英文\n",
    "    final_text = [re.sub(r'[a-zA-Z]',\"\", texts) for texts in temp]\n",
    "    return final_text, getLink\n",
    "\n",
    "def multi_repl(input_str):\n",
    "    trim = ['\\n', '\\r', '\\t', '\\xa0', ' '] + list(enPunc)+list(zhPunc) ## 網頁亂碼+英文標點+中文標點\n",
    "    trim_dict = {pat:'' for pat in trim}\n",
    "    for k in trim_dict:\n",
    "        input_str = input_str.replace(k, trim_dict[k])  ## 把stop words改為空白\n",
    "    return input_str\n",
    "\n",
    "def stopwords():\n",
    "    # Load stop words\n",
    "    f = open('/Users/csti-user/Downloads/NER/stopwords-master/baidu_stopwords.txt','r')\n",
    "    k = f.readlines()\n",
    "    f.close()\n",
    "    stopwords = [lines.strip('\\n') for lines in k]\n",
    "\n",
    "    # 建構stopwords\n",
    "    trim = ['\\n', '\\r', '\\t', '\\xa0'] + list(enPunc)+list(zhPunc) ## 網頁亂碼+英文標點+中文標點\n",
    "    trim = trim + stopwords  # list of all stopwords\n",
    "    return trim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e80f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data Format\n",
    "def ner_combine(ner_pair):\n",
    "    for idx, (word, ner, cat) in enumerate(ner_pair):\n",
    "        if (idx + 1) == len(ner_pair) or (idx + 2) == len(ner_pair):\n",
    "            break\n",
    "        else:  # 直接將組合詞加入原先分詞好的list (ner_pair)\n",
    "            n1_ner = ner_pair[idx+1]\n",
    "            if cat == n1_ner[2]:\n",
    "                n2_ner = ner_pair[idx+2]\n",
    "                if ner == 'ORG' and n1_ner[1] == 'PER':\n",
    "                    new_wd = (word+n1_ner[0],'TITLE+PER',cat)\n",
    "                    del ner_pair[idx:idx+2]\n",
    "                    ner_pair.insert(idx,new_wd)     \n",
    "                elif ner == 'ORG' and n1_ner[1] == 'TITLE' and n2_ner[1] == 'PER':\n",
    "                    new_wd = (word+n1_ner[0]+n2_ner[0],'ORG+TITLE+PER',cat)\n",
    "                    del ner_pair[idx:idx+3]\n",
    "                    ner_pair.insert(idx,new_wd)\n",
    "                    \n",
    "def ner_extract(final_text):\n",
    "    lac = LAC(mode='lac')  \n",
    "    lac.load_customization('custom_dict.txt')  # 自行增加字典\n",
    "    ner_model = lac.run(final_text)\n",
    "\n",
    "    ner_pair = []\n",
    "    training_set = []\n",
    "    trim = stopwords()\n",
    "    for doc, (word, ner) in enumerate(ner_model):\n",
    "        temp = []\n",
    "        for num in range(len(word)):\n",
    "            if word[num] not in trim: # remove stop words\n",
    "                word[num] = multi_repl(word[num])\n",
    "                if word[num] != '' and ner[num] != '' and ner[num] != 'f' and ner[num] != 'nw' and ner[num] != 'm':\n",
    "                    temp.append(word[num])\n",
    "                    ner_pair.append((word[num],ner[num],doc+1))\n",
    "        training_set.append(temp)\n",
    "#     ner_combine(ner_pair)\n",
    "    return ner_pair, training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f98b74e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getLink(google_search_rs):\n",
    "    getLink = []\n",
    "    for idx, link in enumerate(google_search_rs):\n",
    "        getLink.append((idx+1, link))\n",
    "    return getLink"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29100f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Term Frequency Indexing\n",
    "def target_page(ner_pair):\n",
    "    df_all_pair = pd.DataFrame(ner_pair,columns=['words','ner','cat'])\n",
    "########################### dataframe ORG+PER, ORG+TITLE+PER ########################### \n",
    "#     df1 = df_all_pair[df_all_pair['ner'] == 'ORG+PER']\n",
    "#     df2 = df_all_pair[df_all_pair['ner'] == 'ORG+TITLE+PER']\n",
    "#     df_ppl_pair = pd.concat((df1,df2),ignore_index=True)\n",
    "\n",
    "########################### only per #################################\n",
    "    df_ppl_pair = df_all_pair[df_all_pair['ner'] == 'PER']\n",
    "    cnt = [(c,len(df_ppl_pair[df_ppl_pair['cat']==c]['words'].unique())) for c in df_ppl_pair.cat.unique()]\n",
    "\n",
    "    link = pd.DataFrame(getLink(google_search_rs), columns=['doc','link'])\n",
    "    link['cnt'] = 0\n",
    "    for i in cnt:\n",
    "        for idx in link.doc:\n",
    "            if i[0] == idx:\n",
    "                link['cnt'][idx-1] = i[1]\n",
    "    link = link.sort_values(by=['cnt'],ascending=False, ignore_index=True)\n",
    "    link = link.drop(columns=['cnt'])\n",
    "    return link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d48e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "## commands \n",
    "search_word = '振华数据'\n",
    "google_search_rs = pd.read_excel('search3.1.xlsx').url.to_list() # Search2: 振華數據 王雪峰\n",
    "final_text, getLink = getFinalText(google_search_rs)\n",
    "ner_pair, training_set = ner_extract(final_text)\n",
    "print('Target Page of Search 3: \"真溱\"研究员 (TF of PER)')\n",
    "target = target_page(ner_pair)\n",
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd73268",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Search 1 「振華數據」；目標：「CEO 王雪峰」\n",
    "Search 2 「振華數據 王雪峰」；目標：「情報學會」文章 (在第五頁最後一篇)\n",
    "Search 3 「”真溱”研究員」；目標：「情報理論與實踐」文章 (在第一頁第一篇)\n",
    "Search 4 「情報理論與實踐」；目標：「期刊網頁」\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4a7d9f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
